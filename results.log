2020-Apr-12 20:54:50 | RobertaHubInterface(
  (model): RobertaModel(
    (decoder): RobertaEncoder(
      (sentence_encoder): TransformerSentenceEncoder(
        (embed_tokens): Embedding(50265, 1024, padding_idx=1)
        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (classification_heads): ModuleDict()
  )
) | accuracy = [0.33557692307692305] | right ids: ['2', '3', '4', '5', '6', '7', '8', '12', '19', '20', '28', '32', '39', '42', '43', '47', '49', '51', '55', '62', '72', '73', '78', '79', '80', '82', '83', '86', '88', '89', '90', '94', '99', '101', '102', '104', '105', '106', '107', '112', '113', '120', '122', '128', '141', '142', '146', '147', '152', '155', '156', '159', '161', '162', '163', '164', '167', '170', '171', '172', '173', '175', '177', '178', '186', '187', '193', '196', '198', '199', '200', '201', '202', '203', '207', '209', '210', '213', '224', '228', '231', '232', '238', '241', '242', '244', '246', '252', '257', '261', '263', '266', '268', '270', '275', '278', '279', '285', '286', '287', '291', '292', '293', '294', '295', '296', '304', '305', '316', '319', '323', '328', '331', '332', '333', '334', '337', '340', '343', '344', '346', '347', '350', '351', '357', '359', '360', '363', '365', '369', '370', '372', '377', '378', '381', '382', '385', '387', '391', '400', '404', '407', '414', '419', '421', '431', '432', '437', '438', '441', '443', '444', '447', '448', '449', '450', '451', '453', '454', '455', '457', '459', '462', '464', '467', '468', '476', '478', '482', '484', '488', '489', '490', '491', '492', '498', '512', '513', '515', '516', '517', '521', '523', '527', '530', '533', '534', '539', '543', '545', '552', '553', '555', '558', '562', '564', '566', '568', '570', '571', '573', '581', '587', '589', '592', '593', '595', '598', '599', '601', '604', '609', '612', '614', '616', '617', '622', '624', '625', '627', '630', '633', '635', '636', '637', '641', '643', '647', '650', '651', '656', '657', '658', '663', '666', '671', '673', '675', '677', '678', '679', '682', '686', '690', '692', '693', '695', '700', '701', '703', '704', '711', '717', '726', '732', '736', '740', '741', '744', '746', '748', '750', '752', '755', '764', '766', '767', '769', '771', '774', '776', '782', '784', '787', '791', '792', '797', '799', '802', '805', '807', '808', '813', '816', '817', '824', '826', '830', '833', '834', '837', '838', '840', '843', '848', '850', '854', '855', '856', '858', '862', '863', '868', '869', '872', '873', '878', '880', '882', '886', '894', '898', '902', '907', '912', '917', '918', '922', '931', '933', '937', '948', '950', '953', '959', '966', '968', '969', '970', '973', '981', '982', '983', '986', '988', '989', '991', '999', '1003', '1008', '1017', '1018', '1020', '1023', '1024', '1025', '1031', '1036', '1039'] | wrong ids: ['1', '9', '10', '11', '13', '14', '15', '16', '17', '18', '21', '22', '23', '24', '25', '26', '27', '29', '30', '31', '33', '34', '35', '36', '37', '38', '40', '41', '44', '45', '46', '48', '50', '52', '53', '54', '56', '57', '58', '59', '60', '61', '63', '64', '65', '66', '67', '68', '69', '70', '71', '74', '75', '76', '77', '81', '84', '85', '87', '91', '92', '93', '95', '96', '97', '98', '100', '103', '108', '109', '110', '111', '114', '115', '116', '117', '118', '119', '121', '123', '124', '125', '126', '127', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '143', '144', '145', '148', '149', '150', '151', '153', '154', '157', '158', '160', '165', '166', '168', '169', '174', '176', '179', '180', '181', '182', '183', '184', '185', '188', '189', '190', '191', '192', '194', '195', '197', '204', '205', '206', '208', '211', '212', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '225', '226', '227', '229', '230', '233', '234', '235', '236', '237', '239', '240', '243', '245', '247', '248', '249', '250', '251', '253', '254', '255', '256', '258', '259', '260', '262', '264', '265', '267', '269', '271', '272', '273', '274', '276', '277', '280', '281', '282', '283', '284', '288', '289', '290', '297', '298', '299', '300', '301', '302', '303', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '317', '318', '320', '321', '322', '324', '325', '326', '327', '329', '330', '335', '336', '338', '339', '341', '342', '345', '348', '349', '352', '353', '354', '355', '356', '358', '361', '362', '364', '366', '367', '368', '371', '373', '374', '375', '376', '379', '380', '383', '384', '386', '388', '389', '390', '392', '393', '394', '395', '396', '397', '398', '399', '401', '402', '403', '405', '406', '408', '409', '410', '411', '412', '413', '415', '416', '417', '418', '420', '422', '423', '424', '425', '426', '427', '428', '429', '430', '433', '434', '435', '436', '439', '440', '442', '445', '446', '452', '456', '458', '460', '461', '463', '465', '466', '469', '470', '471', '472', '473', '474', '475', '477', '479', '480', '481', '483', '485', '486', '487', '493', '494', '495', '496', '497', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '514', '518', '519', '520', '522', '524', '525', '526', '528', '529', '531', '532', '535', '536', '537', '538', '540', '541', '542', '544', '546', '547', '548', '549', '550', '551', '554', '556', '557', '559', '560', '561', '563', '565', '567', '569', '572', '574', '575', '576', '577', '578', '579', '580', '582', '583', '584', '585', '586', '588', '590', '591', '594', '596', '597', '600', '602', '603', '605', '606', '607', '608', '610', '611', '613', '615', '618', '619', '620', '621', '623', '626', '628', '629', '631', '632', '634', '638', '639', '640', '642', '644', '645', '646', '648', '649', '652', '653', '654', '655', '659', '660', '661', '662', '664', '665', '667', '668', '669', '670', '672', '674', '676', '680', '681', '683', '684', '685', '687', '688', '689', '691', '694', '696', '697', '698', '699', '702', '705', '706', '707', '708', '709', '710', '712', '713', '714', '715', '716', '718', '719', '720', '721', '722', '723', '724', '725', '727', '728', '729', '730', '731', '733', '734', '735', '737', '738', '739', '742', '743', '745', '747', '749', '751', '753', '754', '756', '757', '758', '759', '760', '761', '762', '763', '765', '768', '770', '772', '773', '775', '777', '778', '779', '780', '781', '783', '785', '786', '788', '789', '790', '793', '794', '795', '796', '798', '800', '801', '803', '804', '806', '809', '810', '811', '812', '814', '815', '818', '819', '820', '821', '822', '823', '825', '827', '828', '829', '831', '832', '835', '836', '839', '841', '842', '844', '845', '846', '847', '849', '851', '852', '853', '857', '859', '860', '861', '864', '865', '866', '867', '870', '871', '874', '875', '876', '877', '879', '881', '883', '884', '885', '887', '888', '889', '890', '891', '892', '893', '895', '896', '897', '899', '900', '901', '903', '904', '905', '906', '908', '909', '910', '911', '913', '914', '915', '916', '919', '920', '921', '923', '924', '925', '926', '927', '928', '929', '930', '932', '934', '935', '936', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '949', '951', '952', '954', '955', '956', '957', '958', '960', '961', '962', '963', '964', '965', '967', '971', '972', '974', '975', '976', '977', '978', '979', '980', '984', '985', '987', '990', '992', '993', '994', '995', '996', '997', '998', '1000', '1001', '1002', '1004', '1005', '1006', '1007', '1009', '1010', '1011', '1012', '1013', '1014', '1015', '1016', '1019', '1021', '1022', '1026', '1027', '1028', '1029', '1030', '1032', '1033', '1034', '1035', '1037', '1038', '1040'] | guessed = ['2', '10', '11', '14', '18', '19', '20', '22', '23', '24', '25', '26', '29', '35', '36', '40', '48', '50', '64', '72', '80', '84', '87', '90', '91', '93', '96', '97', '102', '103', '104', '108', '120', '124', '125', '130', '131', '133', '135', '138', '139', '142', '145', '150', '154', '157', '161', '165', '167', '175', '183', '185', '188', '189', '196', '203', '204', '210', '212', '218', '219', '220', '225', '226', '227', '229', '234', '236', '242', '243', '246', '247', '249', '252', '254', '260', '262', '266', '282', '291', '297', '298', '301', '302', '307', '310', '313', '314', '316', '320', '326', '328', '329', '330', '334', '335', '345', '348', '350', '356', '367', '370', '371', '373', '376', '377', '378', '379', '381', '382', '393', '395', '396', '397', '398', '406', '409', '411', '418', '420', '422', '426', '434', '439', '446', '452', '460', '462', '464', '466', '467', '468', '473', '479', '481', '485', '489', '493', '494', '495', '497', '499', '500', '507', '509', '510', '511', '516', '520', '523', '526', '527', '528', '531', '532', '541', '543', '551', '552', '555', '561', '569', '574', '575', '582', '584', '587', '592', '601', '602', '604', '605', '610', '611', '612', '618', '620', '621', '626', '629', '633', '635', '637', '638', '641', '649', '652', '659', '661', '663', '681', '697', '705', '709', '710', '721', '722', '723', '726', '727', '731', '734', '742', '743', '745', '747', '750', '751', '752', '759', '760', '766', '770', '778', '783', '785', '789', '793', '799', '801', '807', '809', '810', '812', '813', '814', '815', '816', '818', '822', '823', '824', '828', '830', '834', '839', '844', '845', '848', '853', '863', '870', '871', '874', '876', '879', '882', '884', '887', '888', '895', '900', '901', '905', '908', '909', '915', '919', '920', '921', '926', '929', '930', '932', '936', '937', '941', '944', '951', '952', '957', '959', '960', '962', '963', '965', '966', '967', '970', '980', '983', '984', '990', '993', '994', '995', '998', '999', '1002', '1011', '1012', '1015', '1020', '1021', '1031', '1032', '1034', '1039', '1040']
