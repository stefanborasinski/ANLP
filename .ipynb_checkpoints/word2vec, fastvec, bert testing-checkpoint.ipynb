{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, random\n",
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "parentdir = \"data\"\n",
    "\n",
    "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
    "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
    "\n",
    "class question:\n",
    "    \n",
    "    def __init__(self,aline):\n",
    "        self.fields=aline\n",
    "        self.anskeys = [\"a)\",\"b)\",\"c)\",\"d)\",\"e)\"]\n",
    "    \n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "    def add_answer(self,fields):\n",
    "        self.fields+=fields[1]\n",
    "\n",
    "\n",
    "    def set_context(self,direction,window=1,target=\"_____\"):\n",
    "        found=-1\n",
    "        direction = direction.lower()[0]\n",
    "        sent_tokens = tokenize(self.get_field(\"question\"))\n",
    "        for i,token in enumerate(sent_tokens):\n",
    "            if token==target:\n",
    "                found=i\n",
    "                break\n",
    "        if found>-1:\n",
    "            question.colnames[\"context \"+direction] = question.colnames.get(\"context \"+direction,len(question.colnames))\n",
    "            if direction == \"l\":\n",
    "                self.fields+=sent_tokens[i-window:i]\n",
    "            if direction == \"r\":\n",
    "                self.fields+=sent_tokens[i+1:i+window+1]\n",
    "    \n",
    "    def make_sentence(self,answer):\n",
    "        q = self.get_field(\"question\")\n",
    "        return q.replace(\"_____\",answer)\n",
    "        \n",
    "        \n",
    "class scc_reader:\n",
    "    \n",
    "    def __init__(self,qs,ans):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "        \n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        question.colnames[\"answer\"] = len(question.colnames)\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "        \n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = scc_reader(questions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc.questions[0].get_field(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc.questions[0].set_context(\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have it from the same source that you are both an orphan and a bachelor and are hi alone in London.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = scc.questions[0].get_field(\"question\")\n",
    "q.replace(\"_____\",\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.',\n",
       " 'crying',\n",
       " 'instantaneously',\n",
       " 'residing',\n",
       " 'matched',\n",
       " 'walking',\n",
       " 'c',\n",
       " 'are',\n",
       " 'alone']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc.questions[0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'question': 1,\n",
       " 'a)': 2,\n",
       " 'b)': 3,\n",
       " 'c)': 4,\n",
       " 'd)': 5,\n",
       " 'e)': 6,\n",
       " 'answer': 8,\n",
       " 'context l': 9,\n",
       " 'context r': 10}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc.questions[0].colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time, timedelta\n",
    "import re, copy, pdb\n",
    "\n",
    "\n",
    "class ResultsLogParser:\n",
    "\n",
    "    def __init__(self, log_path=r\"results.log\"):\n",
    "        self.ops = 0\n",
    "        self.log_path = log_path\n",
    "        self.filtlist = self.get_all()\n",
    "        self.history = []\n",
    "        self.since_when_dict = {\"today\": datetime.combine(date.today(), time()),\n",
    "                                \"yesterday\": datetime.combine(date.today() - timedelta(1), time()),\n",
    "                                \"start\": datetime(2020, 3, 9, 0, 0),\n",
    "                                \"monday\": self._get_weekday(\"monday\"),\n",
    "                                \"tuesday\": self._get_weekday(\"tuesday\"),\n",
    "                                \"wednesday\": self._get_weekday(\"wednesday\"),\n",
    "                                \"thursday\": self._get_weekday(\"thursday\"),\n",
    "                                \"friday\": self._get_weekday(\"friday\"),\n",
    "                                \"saturday\": self._get_weekday(\"saturday\"),\n",
    "                                \"sunday\": self._get_weekday(\"sunday\")}\n",
    "\n",
    "    def get_all(self):\n",
    "        with open(self.log_path, \"r\") as filtlist:\n",
    "            all_results = [line for line in filtlist]\n",
    "        self.filtlist = all_results\n",
    "\n",
    "    def _manage_filtlist(self, filtlist=None):\n",
    "        close = None\n",
    "        if filtlist is None:\n",
    "            filtlist = self.filtlist\n",
    "            if filtlist is not None:\n",
    "                self.history.append(copy.deepcopy(filtlist))\n",
    "            else:\n",
    "                filtlist = open(self.log_path, \"r\")\n",
    "                close = True\n",
    "        return filtlist, close\n",
    "\n",
    "    def _split_line(self, line):\n",
    "        messages = line.split(r' | ')\n",
    "        return messages\n",
    "\n",
    "    def _dirtystring_to_list(self, dirtystring):\n",
    "        m = re.search(r'\\[([^]]*)\\]', dirtystring)\n",
    "        cleanstring = m.group(0)\n",
    "        cleanstring = cleanstring.replace(\"'\", '')\n",
    "        cleanlist = cleanstring.strip('][').split(', ')\n",
    "        results = []\n",
    "        for item in cleanlist:\n",
    "            if item.isnumeric():\n",
    "                item = int(item)\n",
    "            elif item.replace('.', '', 1).isdigit():\n",
    "                item = float(item)\n",
    "            else:\n",
    "                item = str(item)\n",
    "            results.append(item)\n",
    "        return results\n",
    "\n",
    "    def filter_by_model(self, model, filtlist=None):\n",
    "\n",
    "        results = []\n",
    "        filtlist, close = self._manage_filtlist(filtlist=filtlist)\n",
    "\n",
    "        for line in filtlist:\n",
    "            messages = self._split_line(line)\n",
    "            if model in messages[1]:\n",
    "                results.append(line)\n",
    "        if close:\n",
    "            filtlist.close()\n",
    "\n",
    "        self.filtlist = results\n",
    "        self.ops += 1\n",
    "\n",
    "    def undo_steps(self, steps):\n",
    "        if type(steps) == int:\n",
    "            if steps > 0:\n",
    "                if steps <= self.ops:\n",
    "                    self.history = self.history[:self.ops - steps]\n",
    "                    self.ops -= steps\n",
    "                else:\n",
    "                    return print(\"Too many steps back\")\n",
    "            else:\n",
    "                if abs(steps) < self.ops:\n",
    "                    self.history = self.history[:abs(steps)]\n",
    "                    self.ops = abs(steps)\n",
    "                else:\n",
    "                    return print(\"Too many steps forward from start\")\n",
    "        elif type(steps) == str:\n",
    "            if steps in [\"cl\", \"clear\", \"all\"]:\n",
    "                self.get_all()\n",
    "                self.history = []\n",
    "                self.ops = 0\n",
    "                return\n",
    "\n",
    "        self.filtlist = copy.deepcopy(self.history[-1])\n",
    "\n",
    "    def get_as_list(self, keyword, filtlist=None):\n",
    "        results = []\n",
    "        idx = self._find_index(keyword)\n",
    "        filtlist, close = self._manage_filtlist(filtlist=filtlist)\n",
    "        for line in filtlist:\n",
    "            try:\n",
    "                stringlist = self._split_line(line)[idx]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            reslist = self._dirtystring_to_list(stringlist)\n",
    "            results += reslist\n",
    "        if close:\n",
    "            filtlist.close()\n",
    "        try:\n",
    "            return sorted(results)\n",
    "        except TypeError:\n",
    "            return results\n",
    "\n",
    "    def _get_weekday(self, d):\n",
    "        weekdays = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "        twd = weekdays.index(d)  # get number of target weekday\n",
    "        cwd = datetime.today().weekday()  # get number of current weekday\n",
    "        td = abs(abs(cwd - twd) - 7)  # calculate time delta\n",
    "        return datetime.combine(date.today() - timedelta(td), time())\n",
    "\n",
    "    def _find_index(self,\n",
    "                    search_terms):  # find index of message component, added to give more flexibility in log construction\n",
    "        idx = None\n",
    "        if not isinstance(search_terms, list):\n",
    "            search_terms = [search_terms]\n",
    "        found = False\n",
    "        with open(self.log_path, \"r\") as logfile:\n",
    "            while not found:\n",
    "                line = next(logfile)\n",
    "                messages = self._split_line(line)\n",
    "                if len(messages) > 2:\n",
    "                    found = True\n",
    "            for i, message in enumerate(messages):\n",
    "                if any(substring in message for substring in search_terms):\n",
    "                    idx = i\n",
    "                    break\n",
    "        return idx\n",
    "\n",
    "    def filter_by_time(self, *args, hours_ago=None, since_when=None, filtlist=None):\n",
    "        if len(args) == 1:\n",
    "            compare_dt = datetime(\n",
    "                *args[0])  # insert specific datetime in the format expected by datetime ie (2020, 3, 9, 21, 0)\n",
    "        elif hours_ago is not None:\n",
    "            compare_dt = (datetime.now() - timedelta(hours=hours_ago))\n",
    "        elif since_when is not None:\n",
    "            if since_when not in self.since_when_dict.keys():\n",
    "                since_when = \"start\"\n",
    "            compare_dt = self.since_when_dict[since_when]\n",
    "\n",
    "        results = []\n",
    "        filtlist, close = self._manage_filtlist(filtlist=filtlist)\n",
    "\n",
    "        for line in filtlist:\n",
    "            dt = self._split_line(line)[0]\n",
    "            dt = datetime.strptime(dt, '%Y-%b-%d %H:%M:%S')\n",
    "            if dt >= compare_dt:\n",
    "                results.append(line)\n",
    "\n",
    "        if close:\n",
    "            filtlist.close()\n",
    "\n",
    "        self.filtlist = results\n",
    "        self.ops += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp = ResultsLogParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.filter_by_model(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3557692307692308, 0.3557692307692308]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlp.get_as_list(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.undo_steps(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.filter_by_time(since_when=\"Monday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.filter_by_time((2020,3,9,22,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'country-dance',\n",
       " 'to',\n",
       " 'rumours',\n",
       " 'worn-out',\n",
       " 'smoothfaced',\n",
       " 'wormeaten',\n",
       " 'discoloured',\n",
       " 'panelling',\n",
       " 'tissue-paper',\n",
       " 'bellpull',\n",
       " 'fouryearold',\n",
       " 'blood-stained',\n",
       " 'blottingpaper',\n",
       " 'realised',\n",
       " 'policestation',\n",
       " '\"comin\"',\n",
       " 'practised',\n",
       " 'sitting-room',\n",
       " 'dressinggown',\n",
       " 'marvelling',\n",
       " 'ecarte',\n",
       " 'reasoner',\n",
       " 'analyse',\n",
       " 'wellgrown',\n",
       " 'aeroplane',\n",
       " 'pocket-book',\n",
       " 'sepulchre',\n",
       " 'theatres',\n",
       " 'note-book',\n",
       " 'honour',\n",
       " 'motor-car',\n",
       " 'offence',\n",
       " 'drawing-room',\n",
       " 'honourable',\n",
       " 'labour',\n",
       " 'good-natured',\n",
       " 'well-spoken',\n",
       " 'befel',\n",
       " 'first-class',\n",
       " 'good-hearted',\n",
       " 'dressing-table',\n",
       " 'Jamess',\n",
       " 'sittingroom',\n",
       " 'programme',\n",
       " 'centre',\n",
       " 'wicker-work',\n",
       " 'horror-stricken',\n",
       " 'consultingroom',\n",
       " 'battle-cry',\n",
       " 'walking-stick',\n",
       " 'morningroom',\n",
       " 'out-of-the-way',\n",
       " '\"familys\"',\n",
       " 'quarterpast',\n",
       " 'thirty-five',\n",
       " 'small-pox',\n",
       " '\"kings\"',\n",
       " 'smokingroom',\n",
       " 'consulting-room',\n",
       " 'travelled',\n",
       " 'countryhouses',\n",
       " 'good-humoured',\n",
       " 'whipcord',\n",
       " 'illhealth',\n",
       " 'splendour',\n",
       " 'first-fruits',\n",
       " 'fresh-water',\n",
       " 'business-like',\n",
       " 'bell-rope',\n",
       " 'elm-tree',\n",
       " 'unfrequent',\n",
       " 'billiard-room',\n",
       " 'manorhouse',\n",
       " 30000,\n",
       " 'recognising',\n",
       " 'fervour',\n",
       " 'Holmess',\n",
       " 'water-side',\n",
       " '\"wits\"',\n",
       " 'well-merited',\n",
       " 'double-edged',\n",
       " 'organised',\n",
       " 'halfpay',\n",
       " 'half-past',\n",
       " 'labouring',\n",
       " 'sharp-eyed',\n",
       " 'long-legged',\n",
       " 'wide-awake',\n",
       " 'saviour',\n",
       " 'spoilt',\n",
       " 'coalblack',\n",
       " 'night-gown',\n",
       " 'panelled',\n",
       " 'wood-work',\n",
       " 'goodday',\n",
       " 'grown-up',\n",
       " 'half-witted',\n",
       " 'butt-end',\n",
       " 'billiardroom',\n",
       " '\"ladys\"',\n",
       " 'hansoms',\n",
       " 'marvellous',\n",
       " 'humoured',\n",
       " 'pyjamas',\n",
       " 'colour',\n",
       " 'doorkey',\n",
       " 'intentness',\n",
       " 'fellowcountryman',\n",
       " 'bellrope',\n",
       " 'dogcart',\n",
       " 'waitingroom',\n",
       " 'cudgeled',\n",
       " 'favour',\n",
       " 'to-day',\n",
       " 'grey',\n",
       " 2000,\n",
       " '\"sisters\"',\n",
       " 'seven-thirty',\n",
       " 'frockcoat',\n",
       " 'ape-man',\n",
       " 'labours',\n",
       " 'brassbound',\n",
       " 'summer-time',\n",
       " '\"brides\"',\n",
       " '\"horses\"',\n",
       " 'twentyfour',\n",
       " 'travelling',\n",
       " 'diamondshaped',\n",
       " '\"milliners\"',\n",
       " 'jurymen',\n",
       " 'fourwheeled',\n",
       " 'labourers',\n",
       " 'paving-stones',\n",
       " 'sittingrooms',\n",
       " 'long-standing',\n",
       " 'snow-flake',\n",
       " 'neighbours',\n",
       " 'honoured',\n",
       " 'cast-iron',\n",
       " 'broadshouldered',\n",
       " 'to-morrow',\n",
       " 'colourless',\n",
       " 'window-sill',\n",
       " 'slow-witted',\n",
       " 'equalled',\n",
       " 'claw-like',\n",
       " 250,\n",
       " 'dark-lantern',\n",
       " 'dog-cart',\n",
       " 'arm-chair',\n",
       " 'throbbings',\n",
       " 'store-room',\n",
       " 'good-night',\n",
       " 'luncheon-table',\n",
       " 'sister-in-law',\n",
       " 90,\n",
       " 'cross-questioned',\n",
       " 'favourite',\n",
       " 'misdemeanour',\n",
       " 'centred',\n",
       " 'self-contained',\n",
       " 'coatsleeve',\n",
       " 'illused',\n",
       " 'ten-pound',\n",
       " 'enquiring',\n",
       " 'half-full',\n",
       " 'co-operation',\n",
       " 'dining-room',\n",
       " 'second-rate',\n",
       " 800,\n",
       " '60,000',\n",
       " '1,200',\n",
       " 'twenty-one',\n",
       " 'darklantern',\n",
       " 'dinner-party',\n",
       " 'door-step',\n",
       " 'drawingroom',\n",
       " 'pilot-house',\n",
       " 'recognised',\n",
       " 'nought',\n",
       " 'endeavoured',\n",
       " 'school-days',\n",
       " 'neighbourhood',\n",
       " 'to-night',\n",
       " 'warningly',\n",
       " 'tea-table',\n",
       " 'sandy-haired']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlp.get_as_list(\"failwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: data/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os, math, argparse, random\n",
    "from nltk import word_tokenize as tokenize\n",
    "from scc import *\n",
    "import numpy as np\n",
    "\n",
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, trainingdir=r\"data/Holmes_Training_Data\", files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.train()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"ngram trained on {len(self.files)} files\"\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "    def _processline(self, line):\n",
    "        tokens = [\"__START\"] + tokenize(line) + [\"__END\"]\n",
    "        previous = \"__END\"\n",
    "        for token in tokens:\n",
    "            self.unigram[token] = self.unigram.get(token, 0) + 1\n",
    "            current = self.bigram.get(previous, {})\n",
    "            current[token] = current.get(token, 0) + 1\n",
    "            self.bigram[previous] = current\n",
    "            previous = token\n",
    "\n",
    "    def _processfiles(self, verbose=False):\n",
    "        for afile in self.files:\n",
    "            if verbose:\n",
    "                print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "\n",
    "        self.unigram = {k: v / sum(self.unigram.values()) for (k, v) in self.unigram.items()}\n",
    "        self.bigram = {key: {k: v / sum(adict.values()) for (k, v) in adict.items()} for (key, adict) in\n",
    "                       self.bigram.items()}\n",
    "        self.kn = {k: v / sum(self.kn.values()) for (k, v) in self.kn.items()}\n",
    "\n",
    "    def get_prob(self, token, context=\"\", methodparams={}):\n",
    "        if methodparams.get(\"method\", \"unigram\") == \"unigram\":\n",
    "            return self.unigram.get(token, self.unigram.get(\"__UNK\", 0))\n",
    "        else:\n",
    "            if methodparams.get(\"smoothing\", \"kneser-ney\") == \"kneser-ney\":\n",
    "                unidist = self.kn\n",
    "            else:\n",
    "                unidist = self.unigram\n",
    "            bigram = self.bigram.get(context[-1], self.bigram.get(\"__UNK\", {}))\n",
    "            big_p = bigram.get(token, bigram.get(\"__UNK\", 0))\n",
    "            lmbda = bigram[\"__DISCOUNT\"]\n",
    "            uni_p = unidist.get(token, unidist.get(\"__UNK\", 0))\n",
    "            # print(big_p,lmbda,uni_p)\n",
    "            p = big_p + lmbda * uni_p\n",
    "            return p\n",
    "\n",
    "    def compute_prob_line(self, line, methodparams={}):\n",
    "        # this will add _start to the beginning of a line of text\n",
    "        # compute the probability of the line according to the desired model\n",
    "        # and returns probability together with number of tokens\n",
    "\n",
    "        tokens = [\"__START\"] + tokenize(line) + [\"__END\"]\n",
    "        acc = 0\n",
    "        for i, token in enumerate(tokens[1:]):\n",
    "            acc += math.log(self.get_prob(token, tokens[:i + 1], methodparams))\n",
    "        return acc, len(tokens[1:])\n",
    "\n",
    "    def _make_unknowns(self, known=2):\n",
    "        unknown = 0\n",
    "        for (k, v) in list(self.unigram.items()):\n",
    "            if v < known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\", 0) + v\n",
    "        for (k, adict) in list(self.bigram.items()):\n",
    "            for (kk, v) in list(adict.items()):\n",
    "                isknown = self.unigram.get(kk, 0)\n",
    "                if isknown == 0:\n",
    "                    adict[\"__UNK\"] = adict.get(\"__UNK\", 0) + v\n",
    "                    del adict[kk]\n",
    "            isknown = self.unigram.get(k, 0)\n",
    "            if isknown == 0:\n",
    "                del self.bigram[k]\n",
    "                current = self.bigram.get(\"__UNK\", {})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"] = current\n",
    "\n",
    "            else:\n",
    "                self.bigram[k] = adict\n",
    "\n",
    "    def _discount(self, discount=0.75):\n",
    "        # discount each bigram count by a small fixed amount\n",
    "        self.bigram = {k: {kk: value - discount for (kk, value) in adict.items()} for (k, adict) in self.bigram.items()}\n",
    "\n",
    "        # for each word, store the total amount of the discount so that the total is the same\n",
    "        # i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb = len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"] = lamb * discount\n",
    "\n",
    "        # work out kneser-ney unigram probabilities\n",
    "        # count the number of contexts each word has been seen in\n",
    "        self.kn = {}\n",
    "        for (k, adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn[kk] = self.kn.get(kk, 0) + 1\n",
    "\n",
    "def get_training_testing(training_dir=r\"data/Holmes_Training_Data\", split=0.5):\n",
    "    filenames = os.listdir(training_dir)\n",
    "    n = len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n, training_dir))\n",
    "    random.seed(53)  # if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index = int(n * split)\n",
    "    trainingfiles = filenames[:index]\n",
    "    heldoutfiles = filenames[index:]\n",
    "    return trainingfiles, heldoutfiles\n",
    "\n",
    "MAX_FILES = 10\n",
    "\n",
    "training, _ = get_training_testing()\n",
    "mylm = LanguageModel(files=training[:MAX_FILES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18076923076923077\n"
     ]
    }
   ],
   "source": [
    "keys = [\"a)\", \"b)\", \"c)\", \"d)\", \"e)\"]\n",
    "scc = scc_reader()\n",
    "acc = 0\n",
    "correct, incorrect = [], []\n",
    "for question in scc.questions:\n",
    "    scores = []\n",
    "    for key in keys:\n",
    "        answord = question.get_field(key)\n",
    "        s = mylm.get_prob(answord,methodparams={\"smoothing\":\"kneser-ney\"})\n",
    "        scores.append(s)\n",
    "    maxs = max(scores)\n",
    "    idx = np.random.choice(\n",
    "        [i for i, j in enumerate(scores) if j == maxs])  # find index/indices of answers with max score\n",
    "    answer = keys[idx][0]  # answer is first letter of key w/o accompanying bracket\n",
    "    qid = question.get_field(\"id\")\n",
    "    if answer == question.get_field(\"answer\"):\n",
    "        acc += 1\n",
    "        correct.append(qid)\n",
    "    else:\n",
    "        incorrect.append(qid)\n",
    "\n",
    "print(len(correct)/len(scc.questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'unigram'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads('{\"method\":\"unigram\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: e False | I have it from the same source that you are both an orphan and a bachelor and are *walking* alone in London.\n",
      "2: d False | It was furnished partly as a sitting and partly as a bedroom , with flowers arranged *miserably* in every nook and corner.\n",
      "3: d True | As I descended , my old ally , the *guard* , came out of the room and closed the door tightly behind him.\n",
      "4: c True | We got off , *paid* our fare , and the trap rattled back on its way to Leatherhead.\n",
      "5: d True | He held in his hand a *sheet* of blue paper , scrawled over with notes and figures.\n",
      "6: b True | Finally he returned to the pawnbroker's , and , having thumped vigorously upon the pavement with his *stick* two or three times , he went up to the door and knocked.\n",
      "7: b True | There is no *communication* between them , but they all open out into the same corridor.\n",
      "8: c True | He and I seemed to be the only living things between the *huge* arch of the sky and the desert beneath it.\n",
      "9: d False | It may prove the simplest matter in the world , but all the same at first glance this is just a little *brook* , is it not.\n",
      "10: b False | Twelve struck , and one and two and three , and still we sat waiting *intermittently* for whatever might befall.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from scc import *\n",
    "scc = scc_reader()\n",
    "\n",
    "scc = scc_reader()\n",
    "acc = 0\n",
    "correct, incorrect = [], []\n",
    "topk = 50000\n",
    "for question in scc.questions[:10]:\n",
    "    q = question.get_field(\"question\").replace(\"_____\", \"TEMPMASK\")\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    q = q.translate(translator)\n",
    "    q = q.replace(\"TEMPMASK\", \"<mask>\")\n",
    "    rob_masks = roberta.fill_mask(q, topk=topk)\n",
    "    rob_ranks = [mask[2] for mask in rob_masks]\n",
    "    candidates = [question.get_field(ak) for ak in scc.keys]\n",
    "    ans_ranks = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        if candidate in rob_ranks:\n",
    "            ans_ranks.append(rob_ranks.index(candidate))\n",
    "        else:\n",
    "            ans_ranks.append(topk+1)\n",
    "    mins = min(ans_ranks)\n",
    "    idx = np.random.choice([i for i, j in enumerate(ans_ranks) if j == mins])\n",
    "    answer = scc.keys[idx][0]\n",
    "    qid = question.get_field('id')\n",
    "    outcome = answer == question.get_field(\"answer\")\n",
    "    if outcome:\n",
    "        acc += 1\n",
    "        correct.append(qid)\n",
    "    else:\n",
    "        incorrect.append(qid)\n",
    "    print(\n",
    "                f\"{qid}: {answer} {outcome} | {question.make_sentence(question.get_field(scc.keys[idx]), highlight=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
