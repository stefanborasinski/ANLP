{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, random\n",
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "parentdir = \"data\"\n",
    "\n",
    "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
    "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
    "\n",
    "class question:\n",
    "    \n",
    "    def __init__(self,aline):\n",
    "        self.fields=aline\n",
    "        self.anskeys = [\"a)\",\"b)\",\"c)\",\"d)\",\"e)\"]\n",
    "    \n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "    def add_answer(self,fields):\n",
    "        self.fields+=fields[1]\n",
    "\n",
    "\n",
    "    def set_context(self,direction,window=1,target=\"_____\"):\n",
    "        found=-1\n",
    "        direction = direction.lower()[0]\n",
    "        sent_tokens = tokenize(self.get_field(\"question\"))\n",
    "        for i,token in enumerate(sent_tokens):\n",
    "            if token==target:\n",
    "                found=i\n",
    "                break\n",
    "        if found>-1:\n",
    "            question.colnames[\"context \"+direction] = question.colnames.get(\"context \"+direction,len(question.colnames))\n",
    "            if direction == \"l\":\n",
    "                self.fields+=sent_tokens[i-window:i]\n",
    "            if direction == \"r\":\n",
    "                self.fields+=sent_tokens[i+1:i+window+1]\n",
    "    \n",
    "    def make_sentence(self,answer):\n",
    "        q = self.get_field(\"question\")\n",
    "        return q.replace(\"_____\",answer)\n",
    "        \n",
    "        \n",
    "class scc_reader:\n",
    "    \n",
    "    def __init__(self,qs,ans):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "        \n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        question.colnames[\"answer\"] = len(question.colnames)\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "        \n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = scc_reader(questions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc.questions[0].get_field(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc.questions[0].set_context(\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have it from the same source that you are both an orphan and a bachelor and are hi alone in London.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = scc.questions[0].get_field(\"question\")\n",
    "q.replace(\"_____\",\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.',\n",
       " 'crying',\n",
       " 'instantaneously',\n",
       " 'residing',\n",
       " 'matched',\n",
       " 'walking',\n",
       " 'c',\n",
       " 'are',\n",
       " 'alone']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc.questions[0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'question': 1,\n",
       " 'a)': 2,\n",
       " 'b)': 3,\n",
       " 'c)': 4,\n",
       " 'd)': 5,\n",
       " 'e)': 6,\n",
       " 'answer': 8,\n",
       " 'context l': 9,\n",
       " 'context r': 10}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc.questions[0].colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time, timedelta\n",
    "import re, copy, pdb\n",
    "\n",
    "\n",
    "class ResultsLogParser:\n",
    "\n",
    "    def __init__(self, log_path=r\"results.log\"):\n",
    "        self.ops = 0\n",
    "        self.log_path = log_path\n",
    "        self.filtlist = self.get_all()\n",
    "        self.history = []\n",
    "        self.since_when_dict = {\"today\": datetime.combine(date.today(), time()),\n",
    "                                \"yesterday\": datetime.combine(date.today() - timedelta(1), time()),\n",
    "                                \"start\": datetime(2020, 3, 9, 0, 0),\n",
    "                                \"monday\": self._get_weekday(\"monday\"),\n",
    "                                \"tuesday\": self._get_weekday(\"tuesday\"),\n",
    "                                \"wednesday\": self._get_weekday(\"wednesday\"),\n",
    "                                \"thursday\": self._get_weekday(\"thursday\"),\n",
    "                                \"friday\": self._get_weekday(\"friday\"),\n",
    "                                \"saturday\": self._get_weekday(\"saturday\"),\n",
    "                                \"sunday\": self._get_weekday(\"sunday\")}\n",
    "\n",
    "    def get_all(self):\n",
    "        with open(self.log_path, \"r\") as filtlist:\n",
    "            all_results = [line for line in filtlist]\n",
    "        self.filtlist = all_results\n",
    "\n",
    "    def _manage_filtlist(self, filtlist=None):\n",
    "        close = None\n",
    "        if filtlist is None:\n",
    "            filtlist = self.filtlist\n",
    "            if filtlist is not None:\n",
    "                self.history.append(copy.deepcopy(filtlist))\n",
    "            else:\n",
    "                filtlist = open(self.log_path, \"r\")\n",
    "                close = True\n",
    "        return filtlist, close\n",
    "\n",
    "    def _split_line(self, line):\n",
    "        messages = line.split(r' | ')\n",
    "        return messages\n",
    "\n",
    "    def _dirtystring_to_list(self, dirtystring):\n",
    "        m = re.search(r'\\[([^]]*)\\]', dirtystring)\n",
    "        cleanstring = m.group(0)\n",
    "        cleanstring = cleanstring.replace(\"'\", '')\n",
    "        cleanlist = cleanstring.strip('][').split(', ')\n",
    "        results = []\n",
    "        for item in cleanlist:\n",
    "            if item.isnumeric():\n",
    "                item = int(item)\n",
    "            elif item.replace('.', '', 1).isdigit():\n",
    "                item = float(item)\n",
    "            else:\n",
    "                item = str(item)\n",
    "            results.append(item)\n",
    "        return results\n",
    "\n",
    "    def filter_by_model(self, model, filtlist=None):\n",
    "\n",
    "        results = []\n",
    "        filtlist, close = self._manage_filtlist(filtlist=filtlist)\n",
    "\n",
    "        for line in filtlist:\n",
    "            messages = self._split_line(line)\n",
    "            if model in messages[1]:\n",
    "                results.append(line)\n",
    "        if close:\n",
    "            filtlist.close()\n",
    "\n",
    "        self.filtlist = results\n",
    "        self.ops += 1\n",
    "\n",
    "    def undo_steps(self, steps):\n",
    "        if type(steps) == int:\n",
    "            if steps > 0:\n",
    "                if steps <= self.ops:\n",
    "                    self.history = self.history[:self.ops - steps]\n",
    "                    self.ops -= steps\n",
    "                else:\n",
    "                    return print(\"Too many steps back\")\n",
    "            else:\n",
    "                if abs(steps) < self.ops:\n",
    "                    self.history = self.history[:abs(steps)]\n",
    "                    self.ops = abs(steps)\n",
    "                else:\n",
    "                    return print(\"Too many steps forward from start\")\n",
    "        elif type(steps) == str:\n",
    "            if steps in [\"cl\", \"clear\", \"all\"]:\n",
    "                self.get_all()\n",
    "                self.history = []\n",
    "                self.ops = 0\n",
    "                return\n",
    "\n",
    "        self.filtlist = copy.deepcopy(self.history[-1])\n",
    "\n",
    "    def get_as_list(self, keyword, filtlist=None):\n",
    "        results = []\n",
    "        idx = self._find_index(keyword)\n",
    "        filtlist, close = self._manage_filtlist(filtlist=filtlist)\n",
    "        for line in filtlist:\n",
    "            try:\n",
    "                stringlist = self._split_line(line)[idx]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            reslist = self._dirtystring_to_list(stringlist)\n",
    "            results += reslist\n",
    "        if close:\n",
    "            filtlist.close()\n",
    "        try:\n",
    "            return sorted(results)\n",
    "        except TypeError:\n",
    "            return results\n",
    "\n",
    "    def _get_weekday(self, d):\n",
    "        weekdays = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "        twd = weekdays.index(d)  # get number of target weekday\n",
    "        cwd = datetime.today().weekday()  # get number of current weekday\n",
    "        td = abs(abs(cwd - twd) - 7)  # calculate time delta\n",
    "        return datetime.combine(date.today() - timedelta(td), time())\n",
    "\n",
    "    def _find_index(self,\n",
    "                    search_terms):  # find index of message component, added to give more flexibility in log construction\n",
    "        idx = None\n",
    "        if not isinstance(search_terms, list):\n",
    "            search_terms = [search_terms]\n",
    "        found = False\n",
    "        with open(self.log_path, \"r\") as logfile:\n",
    "            while not found:\n",
    "                line = next(logfile)\n",
    "                messages = self._split_line(line)\n",
    "                if len(messages) > 2:\n",
    "                    found = True\n",
    "            for i, message in enumerate(messages):\n",
    "                if any(substring in message for substring in search_terms):\n",
    "                    idx = i\n",
    "                    break\n",
    "        return idx\n",
    "\n",
    "    def filter_by_time(self, *args, hours_ago=None, since_when=None, filtlist=None):\n",
    "        if len(args) == 1:\n",
    "            compare_dt = datetime(\n",
    "                *args[0])  # insert specific datetime in the format expected by datetime ie (2020, 3, 9, 21, 0)\n",
    "        elif hours_ago is not None:\n",
    "            compare_dt = (datetime.now() - timedelta(hours=hours_ago))\n",
    "        elif since_when is not None:\n",
    "            if since_when not in self.since_when_dict.keys():\n",
    "                since_when = \"start\"\n",
    "            compare_dt = self.since_when_dict[since_when]\n",
    "\n",
    "        results = []\n",
    "        filtlist, close = self._manage_filtlist(filtlist=filtlist)\n",
    "\n",
    "        for line in filtlist:\n",
    "            dt = self._split_line(line)[0]\n",
    "            dt = datetime.strptime(dt, '%Y-%b-%d %H:%M:%S')\n",
    "            if dt >= compare_dt:\n",
    "                results.append(line)\n",
    "\n",
    "        if close:\n",
    "            filtlist.close()\n",
    "\n",
    "        self.filtlist = results\n",
    "        self.ops += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp = ResultsLogParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.filter_by_model(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3557692307692308, 0.3557692307692308]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlp.get_as_list(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.undo_steps(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.filter_by_time(since_when=\"Monday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp.filter_by_time((2020,3,9,22,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'country-dance',\n",
       " 'to',\n",
       " 'rumours',\n",
       " 'worn-out',\n",
       " 'smoothfaced',\n",
       " 'wormeaten',\n",
       " 'discoloured',\n",
       " 'panelling',\n",
       " 'tissue-paper',\n",
       " 'bellpull',\n",
       " 'fouryearold',\n",
       " 'blood-stained',\n",
       " 'blottingpaper',\n",
       " 'realised',\n",
       " 'policestation',\n",
       " '\"comin\"',\n",
       " 'practised',\n",
       " 'sitting-room',\n",
       " 'dressinggown',\n",
       " 'marvelling',\n",
       " 'ecarte',\n",
       " 'reasoner',\n",
       " 'analyse',\n",
       " 'wellgrown',\n",
       " 'aeroplane',\n",
       " 'pocket-book',\n",
       " 'sepulchre',\n",
       " 'theatres',\n",
       " 'note-book',\n",
       " 'honour',\n",
       " 'motor-car',\n",
       " 'offence',\n",
       " 'drawing-room',\n",
       " 'honourable',\n",
       " 'labour',\n",
       " 'good-natured',\n",
       " 'well-spoken',\n",
       " 'befel',\n",
       " 'first-class',\n",
       " 'good-hearted',\n",
       " 'dressing-table',\n",
       " 'Jamess',\n",
       " 'sittingroom',\n",
       " 'programme',\n",
       " 'centre',\n",
       " 'wicker-work',\n",
       " 'horror-stricken',\n",
       " 'consultingroom',\n",
       " 'battle-cry',\n",
       " 'walking-stick',\n",
       " 'morningroom',\n",
       " 'out-of-the-way',\n",
       " '\"familys\"',\n",
       " 'quarterpast',\n",
       " 'thirty-five',\n",
       " 'small-pox',\n",
       " '\"kings\"',\n",
       " 'smokingroom',\n",
       " 'consulting-room',\n",
       " 'travelled',\n",
       " 'countryhouses',\n",
       " 'good-humoured',\n",
       " 'whipcord',\n",
       " 'illhealth',\n",
       " 'splendour',\n",
       " 'first-fruits',\n",
       " 'fresh-water',\n",
       " 'business-like',\n",
       " 'bell-rope',\n",
       " 'elm-tree',\n",
       " 'unfrequent',\n",
       " 'billiard-room',\n",
       " 'manorhouse',\n",
       " 30000,\n",
       " 'recognising',\n",
       " 'fervour',\n",
       " 'Holmess',\n",
       " 'water-side',\n",
       " '\"wits\"',\n",
       " 'well-merited',\n",
       " 'double-edged',\n",
       " 'organised',\n",
       " 'halfpay',\n",
       " 'half-past',\n",
       " 'labouring',\n",
       " 'sharp-eyed',\n",
       " 'long-legged',\n",
       " 'wide-awake',\n",
       " 'saviour',\n",
       " 'spoilt',\n",
       " 'coalblack',\n",
       " 'night-gown',\n",
       " 'panelled',\n",
       " 'wood-work',\n",
       " 'goodday',\n",
       " 'grown-up',\n",
       " 'half-witted',\n",
       " 'butt-end',\n",
       " 'billiardroom',\n",
       " '\"ladys\"',\n",
       " 'hansoms',\n",
       " 'marvellous',\n",
       " 'humoured',\n",
       " 'pyjamas',\n",
       " 'colour',\n",
       " 'doorkey',\n",
       " 'intentness',\n",
       " 'fellowcountryman',\n",
       " 'bellrope',\n",
       " 'dogcart',\n",
       " 'waitingroom',\n",
       " 'cudgeled',\n",
       " 'favour',\n",
       " 'to-day',\n",
       " 'grey',\n",
       " 2000,\n",
       " '\"sisters\"',\n",
       " 'seven-thirty',\n",
       " 'frockcoat',\n",
       " 'ape-man',\n",
       " 'labours',\n",
       " 'brassbound',\n",
       " 'summer-time',\n",
       " '\"brides\"',\n",
       " '\"horses\"',\n",
       " 'twentyfour',\n",
       " 'travelling',\n",
       " 'diamondshaped',\n",
       " '\"milliners\"',\n",
       " 'jurymen',\n",
       " 'fourwheeled',\n",
       " 'labourers',\n",
       " 'paving-stones',\n",
       " 'sittingrooms',\n",
       " 'long-standing',\n",
       " 'snow-flake',\n",
       " 'neighbours',\n",
       " 'honoured',\n",
       " 'cast-iron',\n",
       " 'broadshouldered',\n",
       " 'to-morrow',\n",
       " 'colourless',\n",
       " 'window-sill',\n",
       " 'slow-witted',\n",
       " 'equalled',\n",
       " 'claw-like',\n",
       " 250,\n",
       " 'dark-lantern',\n",
       " 'dog-cart',\n",
       " 'arm-chair',\n",
       " 'throbbings',\n",
       " 'store-room',\n",
       " 'good-night',\n",
       " 'luncheon-table',\n",
       " 'sister-in-law',\n",
       " 90,\n",
       " 'cross-questioned',\n",
       " 'favourite',\n",
       " 'misdemeanour',\n",
       " 'centred',\n",
       " 'self-contained',\n",
       " 'coatsleeve',\n",
       " 'illused',\n",
       " 'ten-pound',\n",
       " 'enquiring',\n",
       " 'half-full',\n",
       " 'co-operation',\n",
       " 'dining-room',\n",
       " 'second-rate',\n",
       " 800,\n",
       " '60,000',\n",
       " '1,200',\n",
       " 'twenty-one',\n",
       " 'darklantern',\n",
       " 'dinner-party',\n",
       " 'door-step',\n",
       " 'drawingroom',\n",
       " 'pilot-house',\n",
       " 'recognised',\n",
       " 'nought',\n",
       " 'endeavoured',\n",
       " 'school-days',\n",
       " 'neighbourhood',\n",
       " 'to-night',\n",
       " 'warningly',\n",
       " 'tea-table',\n",
       " 'sandy-haired']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlp.get_as_list(\"failwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: data/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os, math, argparse, random\n",
    "from nltk import word_tokenize as tokenize\n",
    "from scc import *\n",
    "import numpy as np\n",
    "\n",
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, trainingdir=r\"data/Holmes_Training_Data\", files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.train()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"ngram trained on {len(self.files)} files\"\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "    def _processline(self, line):\n",
    "        tokens = [\"__START\"] + tokenize(line) + [\"__END\"]\n",
    "        previous = \"__END\"\n",
    "        for token in tokens:\n",
    "            self.unigram[token] = self.unigram.get(token, 0) + 1\n",
    "            current = self.bigram.get(previous, {})\n",
    "            current[token] = current.get(token, 0) + 1\n",
    "            self.bigram[previous] = current\n",
    "            previous = token\n",
    "\n",
    "    def _processfiles(self, verbose=False):\n",
    "        for afile in self.files:\n",
    "            if verbose:\n",
    "                print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "\n",
    "        self.unigram = {k: v / sum(self.unigram.values()) for (k, v) in self.unigram.items()}\n",
    "        self.bigram = {key: {k: v / sum(adict.values()) for (k, v) in adict.items()} for (key, adict) in\n",
    "                       self.bigram.items()}\n",
    "        self.kn = {k: v / sum(self.kn.values()) for (k, v) in self.kn.items()}\n",
    "\n",
    "    def get_prob(self, token, context=\"\", methodparams={}):\n",
    "        if methodparams.get(\"method\", \"unigram\") == \"unigram\":\n",
    "            return self.unigram.get(token, self.unigram.get(\"__UNK\", 0))\n",
    "        else:\n",
    "            if methodparams.get(\"smoothing\", \"kneser-ney\") == \"kneser-ney\":\n",
    "                unidist = self.kn\n",
    "            else:\n",
    "                unidist = self.unigram\n",
    "            bigram = self.bigram.get(context[-1], self.bigram.get(\"__UNK\", {}))\n",
    "            big_p = bigram.get(token, bigram.get(\"__UNK\", 0))\n",
    "            lmbda = bigram[\"__DISCOUNT\"]\n",
    "            uni_p = unidist.get(token, unidist.get(\"__UNK\", 0))\n",
    "            # print(big_p,lmbda,uni_p)\n",
    "            p = big_p + lmbda * uni_p\n",
    "            return p\n",
    "\n",
    "    def compute_prob_line(self, line, methodparams={}):\n",
    "        # this will add _start to the beginning of a line of text\n",
    "        # compute the probability of the line according to the desired model\n",
    "        # and returns probability together with number of tokens\n",
    "\n",
    "        tokens = [\"__START\"] + tokenize(line) + [\"__END\"]\n",
    "        acc = 0\n",
    "        for i, token in enumerate(tokens[1:]):\n",
    "            acc += math.log(self.get_prob(token, tokens[:i + 1], methodparams))\n",
    "        return acc, len(tokens[1:])\n",
    "\n",
    "    def _make_unknowns(self, known=2):\n",
    "        unknown = 0\n",
    "        for (k, v) in list(self.unigram.items()):\n",
    "            if v < known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\", 0) + v\n",
    "        for (k, adict) in list(self.bigram.items()):\n",
    "            for (kk, v) in list(adict.items()):\n",
    "                isknown = self.unigram.get(kk, 0)\n",
    "                if isknown == 0:\n",
    "                    adict[\"__UNK\"] = adict.get(\"__UNK\", 0) + v\n",
    "                    del adict[kk]\n",
    "            isknown = self.unigram.get(k, 0)\n",
    "            if isknown == 0:\n",
    "                del self.bigram[k]\n",
    "                current = self.bigram.get(\"__UNK\", {})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"] = current\n",
    "\n",
    "            else:\n",
    "                self.bigram[k] = adict\n",
    "\n",
    "    def _discount(self, discount=0.75):\n",
    "        # discount each bigram count by a small fixed amount\n",
    "        self.bigram = {k: {kk: value - discount for (kk, value) in adict.items()} for (k, adict) in self.bigram.items()}\n",
    "\n",
    "        # for each word, store the total amount of the discount so that the total is the same\n",
    "        # i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb = len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"] = lamb * discount\n",
    "\n",
    "        # work out kneser-ney unigram probabilities\n",
    "        # count the number of contexts each word has been seen in\n",
    "        self.kn = {}\n",
    "        for (k, adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn[kk] = self.kn.get(kk, 0) + 1\n",
    "\n",
    "def get_training_testing(training_dir=r\"data/Holmes_Training_Data\", split=0.5):\n",
    "    filenames = os.listdir(training_dir)\n",
    "    n = len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n, training_dir))\n",
    "    random.seed(53)  # if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index = int(n * split)\n",
    "    trainingfiles = filenames[:index]\n",
    "    heldoutfiles = filenames[index:]\n",
    "    return trainingfiles, heldoutfiles\n",
    "\n",
    "MAX_FILES = 10\n",
    "\n",
    "training, _ = get_training_testing()\n",
    "mylm = LanguageModel(files=training[:MAX_FILES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18076923076923077\n"
     ]
    }
   ],
   "source": [
    "keys = [\"a)\", \"b)\", \"c)\", \"d)\", \"e)\"]\n",
    "scc = scc_reader()\n",
    "acc = 0\n",
    "correct, incorrect = [], []\n",
    "for question in scc.questions:\n",
    "    scores = []\n",
    "    for key in keys:\n",
    "        answord = question.get_field(key)\n",
    "        s = mylm.get_prob(answord,methodparams={\"smoothing\":\"kneser-ney\"})\n",
    "        scores.append(s)\n",
    "    maxs = max(scores)\n",
    "    idx = np.random.choice(\n",
    "        [i for i, j in enumerate(scores) if j == maxs])  # find index/indices of answers with max score\n",
    "    answer = keys[idx][0]  # answer is first letter of key w/o accompanying bracket\n",
    "    qid = question.get_field(\"id\")\n",
    "    if answer == question.get_field(\"answer\"):\n",
    "        acc += 1\n",
    "        correct.append(qid)\n",
    "    else:\n",
    "        incorrect.append(qid)\n",
    "\n",
    "print(len(correct)/len(scc.questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'unigram'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads('{\"method\":\"unigram\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DYNMT10.TXT',\n",
       " '09TOM10.TXT',\n",
       " 'PRSIT10.TXT',\n",
       " 'NWIND10.TXT',\n",
       " 'BDAPH10.TXT',\n",
       " 'COTRT10.TXT',\n",
       " 'POISN10.TXT',\n",
       " 'RNDBY10.TXT',\n",
       " 'LAMEP10.TXT',\n",
       " 'OAKDA10.TXT',\n",
       " 'TGAMT210.TXT',\n",
       " 'MSBIM10.TXT',\n",
       " 'ALAMO10.TXT',\n",
       " 'FRCUP10.TXT',\n",
       " 'NABBY10.TXT',\n",
       " 'THEEU10.TXT',\n",
       " 'SNYBK10.TXT',\n",
       " 'TLTTF10.TXT',\n",
       " 'STROQ10.TXT',\n",
       " 'LDASC10.TXT',\n",
       " 'EMMA10.TXT',\n",
       " 'DOMBY10.TXT',\n",
       " 'THBTC10.TXT',\n",
       " 'ZENDA10.TXT',\n",
       " 'AOFBT10.TXT',\n",
       " 'HARDT10.TXT',\n",
       " 'TCNTR10.TXT',\n",
       " 'CPRFD10.TXT',\n",
       " 'DOLIT10.TXT',\n",
       " 'SWGEM10.TXT',\n",
       " 'WRAIR10.TXT',\n",
       " 'TWOHE10.TXT',\n",
       " 'SILVS10.TXT',\n",
       " 'TESS10.TXT',\n",
       " 'OURNG10.TXT',\n",
       " 'ALTDD10.TXT',\n",
       " '11WOZ10.TXT',\n",
       " 'TWBGD10.TXT',\n",
       " 'RBCRU10.TXT',\n",
       " 'LNDLR10.TXT',\n",
       " 'JUDE11.TXT',\n",
       " 'NOTUN11.TXT',\n",
       " 'SIOUX10.TXT',\n",
       " 'KDNPD10.TXT',\n",
       " 'HRLND10.TXT',\n",
       " 'SJV0410.TXT',\n",
       " 'MAYRC10.TXT',\n",
       " 'SHLIN10.TXT',\n",
       " 'STRKM10.TXT',\n",
       " '7GABL10.TXT',\n",
       " 'ENYEW10.TXT',\n",
       " 'LDORT10.TXT',\n",
       " 'TBSCC10.TXT',\n",
       " 'RAFLS10.TXT',\n",
       " 'AMRNT10.TXT',\n",
       " '2DINA10.TXT',\n",
       " 'RDFRY10.TXT',\n",
       " 'DTROY10.TXT',\n",
       " 'SLGRL10.TXT',\n",
       " 'SUMMR10.TXT',\n",
       " 'ANDES10.TXT',\n",
       " 'TOUCH10.TXT',\n",
       " 'HBOSS10.TXT',\n",
       " '21TOM10.TXT',\n",
       " 'JUNGL10.TXT',\n",
       " 'CONFI10.TXT',\n",
       " 'MORLL10.TXT',\n",
       " 'MDKNG10.TXT',\n",
       " 'NTNTN10.TXT',\n",
       " 'IVNHO12.TXT',\n",
       " 'OTORN10.TXT',\n",
       " 'VIFRY10.TXT',\n",
       " 'BLAGO10.TXT',\n",
       " 'TREAS10.TXT',\n",
       " 'CHOUR10.TXT',\n",
       " 'DSTHJ10.TXT',\n",
       " 'CKDEC10.TXT',\n",
       " 'PHIL410.TXT',\n",
       " '1ARGN10.TXT',\n",
       " 'ISLNI10.TXT',\n",
       " 'TMANS10.TXT',\n",
       " '1ADAM10.TXT',\n",
       " 'MOON10.TXT',\n",
       " 'RBCR210.TXT',\n",
       " '10WOZ10.TXT',\n",
       " 'BICAR10.TXT',\n",
       " 'RD2OZ10.TXT',\n",
       " 'MANSF10.TXT',\n",
       " 'HHOTL10.TXT',\n",
       " 'HDKNK10.TXT',\n",
       " 'KIDS110.TXT',\n",
       " 'WIZOZ10.TXT',\n",
       " 'ACHOE10.TXT',\n",
       " 'CHILC10.TXT',\n",
       " 'DWARE10.TXT',\n",
       " 'TGAMT10.TXT',\n",
       " 'MNBTW10.TXT',\n",
       " 'FIVIT10.TXT',\n",
       " 'ECORE10.TXT',\n",
       " 'FSRGS10.TXT',\n",
       " 'WOODL10.TXT',\n",
       " 'PETER16.TXT',\n",
       " 'MRAMN10.TXT',\n",
       " 'PORAP10.TXT',\n",
       " 'MNSTR10.TXT',\n",
       " 'MSBID10.TXT',\n",
       " 'WWILL10.TXT',\n",
       " 'SNOWI10.TXT',\n",
       " 'RDDSK10.TXT',\n",
       " 'AMNTS10.TXT',\n",
       " 'TWPOP10.TXT',\n",
       " 'OOTMA10.TXT',\n",
       " 'MAGI10.TXT',\n",
       " '10EVM10.TXT',\n",
       " 'TMUCK10.TXT',\n",
       " 'DRDAY10.TXT',\n",
       " '5DFRE10.TXT',\n",
       " 'ANVER10.TXT',\n",
       " 'RMEND10.TXT',\n",
       " 'REZNV10.TXT',\n",
       " 'MGOTS10.TXT',\n",
       " 'RNDUP10.TXT',\n",
       " '1MLKD11.TXT',\n",
       " 'TNGLW10.TXT',\n",
       " 'MSTNF10.TXT',\n",
       " 'AHERO10.TXT',\n",
       " 'BCPTV10.TXT',\n",
       " 'SSEAS10.TXT',\n",
       " 'DUGLAS11.TXT',\n",
       " '3ELPH10.TXT',\n",
       " '04TOM10.TXT',\n",
       " 'PALIN10.TXT',\n",
       " 'BADGE10.TXT',\n",
       " '05TOM10.TXT',\n",
       " 'ENOCH10.TXT',\n",
       " 'YLFRY10.TXT',\n",
       " 'AMBAS10.TXT',\n",
       " 'TRED110.TXT',\n",
       " 'HBOOK10.TXT',\n",
       " 'WNLAW10.TXT',\n",
       " 'PCRCS10.TXT',\n",
       " 'LPRSS11.TXT',\n",
       " 'SCARP10.TXT',\n",
       " 'MAROG10.TXT',\n",
       " 'BEQST11.TXT',\n",
       " 'TSOPR09.TXT',\n",
       " 'YANKE11.TXT',\n",
       " 'AMATC10.TXT',\n",
       " 'TWILS10.TXT',\n",
       " 'BMINE10.TXT',\n",
       " 'TKOGR10.TXT',\n",
       " 'BRLAM10.TXT',\n",
       " 'GSILX10.TXT',\n",
       " 'TARZ610.TXT',\n",
       " 'PTPED10.TXT',\n",
       " 'GOLDR10.TXT',\n",
       " 'FRHNT10.TXT',\n",
       " 'MARIA10.TXT',\n",
       " 'NATIV10.TXT',\n",
       " 'WUTHR10.TXT',\n",
       " 'KRSON10.TXT',\n",
       " 'TCHMS10.TXT',\n",
       " 'CEVEN10.TXT',\n",
       " 'LPRIN10.TXT',\n",
       " 'TONOB10.TXT',\n",
       " 'TETHR10.TXT',\n",
       " 'PRESC10.TXT',\n",
       " 'YNKGP10.TXT',\n",
       " 'TCOTH10.TXT',\n",
       " 'MARKT10.TXT',\n",
       " '1DONQ10.TXT',\n",
       " 'AESOPA10.TXT',\n",
       " 'TOTAM10.TXT',\n",
       " 'DGRAY10.TXT',\n",
       " 'TOM2010.TXT',\n",
       " 'ANNE11.TXT',\n",
       " 'VILWT10.TXT',\n",
       " 'HYDEA10.TXT',\n",
       " '2000010.TXT',\n",
       " '12TOM10.TXT',\n",
       " 'FLIRT10.TXT',\n",
       " 'TDITW10.TXT',\n",
       " '3BOAT10.TXT',\n",
       " 'KDSTA10.TXT',\n",
       " 'CAROL11.TXT',\n",
       " 'POLST10.TXT',\n",
       " 'SCCAR10.TXT',\n",
       " 'DPROF10.TXT',\n",
       " 'NDRDG10.TXT',\n",
       " 'MPOOL10.TXT',\n",
       " 'LOSTC10.TXT',\n",
       " 'MAIDM10.TXT',\n",
       " 'DGOLD10.TXT',\n",
       " 'SOLDF10.TXT',\n",
       " 'LVGRO10.TXT',\n",
       " 'FANFB10.TXT',\n",
       " 'HNTSK10.TXT',\n",
       " 'ARABN11.TXT',\n",
       " '6DFRE10.TXT',\n",
       " 'LOSTW10.TXT',\n",
       " 'RKING10.TXT',\n",
       " 'PNOCO10.TXT',\n",
       " 'LOCTY10.TXT',\n",
       " 'RHUDS10.TXT',\n",
       " 'WELND10.TXT',\n",
       " 'BADGE10A.TXT',\n",
       " 'BEAST10.TXT',\n",
       " 'LORNA10.TXT',\n",
       " 'TDUST10.TXT',\n",
       " 'GNDIN10.TXT',\n",
       " 'GGPAN10.TXT',\n",
       " 'LAIDR10.TXT',\n",
       " '08WOZ10.TXT',\n",
       " 'LESMS10.TXT',\n",
       " 'UTOMC10.TXT',\n",
       " 'BATLF10.TXT',\n",
       " 'FORGD10.TXT',\n",
       " 'MONST10.TXT',\n",
       " 'TRRYE10.TXT',\n",
       " 'AGENT10.TXT',\n",
       " 'AWILL10.TXT',\n",
       " 'SMARN10.TXT',\n",
       " 'BUNNR10.TXT',\n",
       " 'PLGRM10.TXT',\n",
       " 'TSOTS10.TXT',\n",
       " 'PRCUR10.TXT',\n",
       " 'SAWY310.TXT',\n",
       " 'GLDNA10.TXT',\n",
       " 'OZLAND10.TXT',\n",
       " 'PWPRS10.TXT',\n",
       " 'TCOST10.TXT',\n",
       " 'ALEXB10.TXT',\n",
       " 'VOOUT10.TXT',\n",
       " 'STRNG10.TXT',\n",
       " 'STHOL10.TXT',\n",
       " 'TCROS10.TXT',\n",
       " 'TROLL10.TXT',\n",
       " 'CIRCS10.TXT',\n",
       " 'BLNTR10.TXT',\n",
       " 'SANDB10.TXT',\n",
       " 'RCRIM10.TXT',\n",
       " 'JOTHB10.TXT',\n",
       " 'WTSLW10.TXT',\n",
       " 'PHIDL10.TXT',\n",
       " '1BOON10.TXT',\n",
       " 'HIPHO10.TXT',\n",
       " 'LMISS11.TXT',\n",
       " 'INDAY10.TXT',\n",
       " 'THEAM10.TXT',\n",
       " 'JBARL10.TXT',\n",
       " 'OZMOZ10.TXT',\n",
       " 'FWALD10.TXT',\n",
       " '1DFRE10.TXT',\n",
       " 'TARZ310.TXT',\n",
       " 'LENOX10.TXT',\n",
       " 'NVOYG10.TXT',\n",
       " 'EMCTY10.TXT',\n",
       " 'AFOST10.TXT',\n",
       " 'BDLIT10.TXT',\n",
       " 'WWALS10.TXT',\n",
       " 'STIVE10.TXT']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[:None]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
